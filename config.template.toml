###################### OpenHands Configuration Example ######################
#
# All settings have default values, so you only need to uncomment and
# modify what you want to change
# The fields within each section are sorted in alphabetical order.
#
##############################################################################

#################################### Core ####################################
# General core configurations
##############################################################################
[core]
# API key for E2B
#e2b_api_key = ""

# Base path for the workspace
workspace_base = "./workspace"

# Cache directory path
#cache_dir = "/tmp/cache"

# Debugging enabled
#debug = false

# Disable color in terminal output
#disable_color = false

# Enable saving and restoring the session when run from CLI
#enable_cli_session = false

# File store path
#file_store_path = "/tmp/file_store"

# File store type
#file_store = "memory"

# List of allowed file extensions for uploads
#file_uploads_allowed_extensions = [".*"]

# Maximum file size for uploads, in megabytes
#file_uploads_max_file_size_mb = 0

# Maximum budget per task, 0.0 means no limit
#max_budget_per_task = 0.0

# Maximum number of iterations
#max_iterations = 100

# Path to mount the workspace in the sandbox
#workspace_mount_path_in_sandbox = "/workspace"

# Path to mount the workspace
#workspace_mount_path = ""

# Path to rewrite the workspace mount path to
#workspace_mount_rewrite = ""

# Run as openhands
#run_as_openhands = true

# Runtime environment
#runtime = "eventstream"

# Name of the default agent
#default_agent = "CodeActAgent"

# JWT secret for authentication
#jwt_secret = ""

# Restrict file types for file uploads
#file_uploads_restrict_file_types = false

# List of allowed file extensions for uploads
#file_uploads_allowed_extensions = [".*"]

#################################### LLM #####################################
# Configuration for LLM models (group name starts with 'llm')
# use 'llm' for the default LLM config
##############################################################################
[llm]
# AWS access key ID
#aws_access_key_id = ""

# AWS region name
#aws_region_name = ""

# AWS secret access key
#aws_secret_access_key = ""

# API key to use
api_key = "your-api-key"

# API base URL
#base_url = ""

# API version
#api_version = ""

# Cost per input token
#input_cost_per_token = 0.0

# Cost per output token
#output_cost_per_token = 0.0

# Custom LLM provider
#custom_llm_provider = ""

# Embedding API base URL
#embedding_base_url = ""

# Embedding deployment name
#embedding_deployment_name = ""

# Embedding model to use
embedding_model = "local"

# Maximum number of characters in an observation's content
#max_message_chars = 10000

# Maximum number of input tokens
#max_input_tokens = 0

# Maximum number of output tokens
#max_output_tokens = 0

# Model to use
model = "gpt-4o"

# Number of retries to attempt when an operation fails with the LLM.
# Increase this value to allow more attempts before giving up
#num_retries = 8

# Maximum wait time (in seconds) between retry attempts
# This caps the exponential backoff to prevent excessively long
#retry_max_wait = 120

# Minimum wait time (in seconds) between retry attempts
# This sets the initial delay before the first retry
#retry_min_wait = 15

# Multiplier for exponential backoff calculation
# The wait time increases by this factor after each failed attempt
# A value of 2.0 means each retry waits twice as long as the previous one
#retry_multiplier = 2.0

# Drop any unmapped (unsupported) params without causing an exception
#drop_params = false

# Using the prompt caching feature if provided by the LLM and supported
#caching_prompt = true

# Base URL for the OLLAMA API
#ollama_base_url = ""

# Temperature for the API
#temperature = 0.0

# Timeout for the API
#timeout = 0

# Top p for the API
#top_p = 1.0

# If model is vision capable, this option allows to disable image processing (useful for cost reduction).
#disable_vision = true

################################### Router ###################################
# litellm Router configuration
# https://litellm.vercel.app/docs/routing
##############################################################################

# The "router_config"section can only occur once in the config.toml file!
#[router_config]

# The default model to use for routing, which must be part of the "models" entries below.
#default_model = "gpt-4o"

# The strategy to use for routing (e.g., usage-based-routing, cost-based-routing)
#routing_strategy = "usage-based-routing"

# Number of retries to attempt when an operation fails
#num_retries = 3

# waits min. 5s before retrying request
#retry_after = 15

# cooldown model if it fails > 1 call in a minute
#allowed_fails = 1

# cooldown the deployment for 100 seconds if it num_fails > allowed_fails
#cooldown_time = 100

# raise timeout error if call takes > 240s
#timeout = 240

# Whether to cache responses for reuse
#cache_responses = false

# Additional cache configuration options, see documentation for details
#[router_config.cache_kwargs]

#[router_config.retry_policy]
#exceptions_to_retry = [ "RateLimitError",]  # List of exceptions to retry on
#max_retries = 8  # Maximum number of retries for the specified exceptions
#retry_after = 15  # Time to wait before the first retry for the specified exceptions
#retry_after_multiplier = 2  # Multiplier for exponential backoff calculation

# --- Important Notes ---
# - The model entries below are just examples, you can add, edit or remove these as needed.
# - For each model entry, there is a corresponding set of "litellm_params" section,
#   which must be specified in the exact same syntax as shown below.
# - The syntax with the brackets [[ ]] is mandatory, even if there is only one model entry,
#   or the whole config file may not be loaded!
# - The model name must be unique in the list!
# - The model name can be the same as the model alias.
# - The empty lines are only added for readability, they are not required.
# ----

# --- 1st model example: Azure OpenAI

# See: https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits
# See: https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models

#[[router_config.models]]
# An alias name for this model, must be unique in the list
#model_name = "gpt-4-vision-enhancements"

# Each model can have its own set of parameters, note the single [] syntax!
#[router_config.models.litellm_params]
# The actual model name as per the provider's API
#model = "azure/gpt-4-vision"
# API key for the provider
#api_key = "AZURE_API_KEY"
# Base URL for the API
#base_url = "https://gpt-4-vision-resource.openai.azure.com/openai/deployments/gpt-4-vision/extensions/"
# some providers may require/offer datasources for deployments (optional)
#[[router_config.models.litellm_params.dataSources]]
# Type of data source
#type = "AzureComputerVision"
#[router_config.models.litellm_params.dataSources.parameters]
#endpoint = "AZURE_VISION_ENHANCE_ENDPOINT"  # Endpoint for the data source
#key = "AZURE_VISION_ENHANCE_KEY"            # Key for the data source
#rpm = 900           # Requests per minute limit
#tpm = 150000        # Tokens per minute limit

# --- 2nd model example: OpenRouter w/ Qwen 2.5

# See: https://openrouter.ai/models/

#[[router_config.models]]
#model_name = "or-qwen-2.5"

#[router_config.models.litellm_params]
#model = "openrouter/qwen/qwen-2.5-72b-instruct"
#api_base = "https://openrouter.ai/api/v1" # Optional with litellm if model starts with "openrouter/"
#api_key = "sk-or-v1-xxx"
# Note: all other models may have these parameters, which are optional,
# but are only mentioned here for brevity. These are example values!
#max_retries = 3       # Number of retries to attempt when an operation fails
#max_tokens = 4000     # Maximum number of output tokens
#tpm = 10000           # Tokens per minute limit
#rpm = 6               # Requests per minute limit
#timeout = 120         # The timeout set in router is for the entire length of the call

# --- 3rd model example: Anthropic w/ Sonnet 3.5

#[[router_config.models]]
#model_name = "ant-sonnet-3.5"

#[router_config.models.litellm_params]
#model = "anthropic/claude-3-5-sonnet-20240620"
#api_key = "sk-ant-xxx"
#api_base not needed

# --- 4th model example: OpenAI w/ GPT-4o

# See: https://platform.openai.com/docs/models

#[[router_config.models]]
#model_name = "gpt-4o"
#[router_config.models.litellm_params]
# This has 128K context length with 16K max output tokens!
# "Dynamic model continuously updated"
#model = "openai/chatgpt-4o-latest"
#api_key = "sk-proj-xxx"
#api_base not needed
#max_tokens = 16384
#rpm = 6

# --- 5th model example: Groq w/ Llama 3.2 90B

#[[router_config.models]]
#model_name = "groq-lama-3.2-90b"
#[router_config.models.litellm_params]
#model = "groq/llama-3.2-90b-text-preview"
#api_base = "https://api.groq.com/openai/v1"
#api_key = "gsk_xxx"



#################################### Agent ###################################
# Configuration for agents (group name starts with 'agent')
# Use 'agent' for the default agent config
# otherwise, group name must be `agent.<agent_name>` (case-sensitive), e.g.
# agent.CodeActAgent
##############################################################################
[agent]
# Name of the micro agent to use for this agent
#micro_agent_name = ""

# Memory enabled
#memory_enabled = false

# Memory maximum threads
#memory_max_threads = 2

# LLM config group to use
#llm_config = 'your-llm-config-group'

[agent.RepoExplorerAgent]
# Example: use a cheaper model for RepoExplorerAgent to reduce cost, especially
# useful when an agent doesn't demand high quality but uses a lot of tokens
llm_config = 'gpt3'

#################################### Sandbox ###################################
# Configuration for the sandbox
##############################################################################
[sandbox]
# Sandbox timeout in seconds
#timeout = 120

# Sandbox user ID
#user_id = 1000

# Container image to use for the sandbox
#base_container_image = "nikolaik/python-nodejs:python3.11-nodejs22"

# Use host network
#use_host_network = false

# Enable auto linting after editing
#enable_auto_lint = false

# Whether to initialize plugins
#initialize_plugins = true

# Extra dependencies to install in the runtime image
#runtime_extra_deps = ""

# Environment variables to set at the launch of the runtime
#runtime_startup_env_vars = {}

# BrowserGym environment to use for evaluation
#browsergym_eval_env = ""

#################################### Security ###################################
# Configuration for security features
##############################################################################
[security]

# Enable confirmation mode
#confirmation_mode = false

# The security analyzer to use
#security_analyzer = ""

#################################### Eval ####################################
# Configuration for the evaluation, please refer to the specific evaluation
# plugin for the available options
##############################################################################
